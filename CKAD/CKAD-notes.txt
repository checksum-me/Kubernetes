//CKAD Course credits to kodekloud - awesome course providing the necessary (as seen below) for CKAD
//kubectl [command] [TYPE] [NAME] -o <output_format>
//Here are some of the commonly used formats:
//    -o jsonOutput a JSON formatted API object.
//    -o namePrint only the resource name and nothing else.
//    -o wideOutput in the plain-text format with any additional information.
//    -o yamlOutput a YAML formatted API object.

//To view all the resources in the default namespace
k get all

//To view all the resources in all the namespace
k get all -A

//List all the api resources and their api
k api-resources

//List all the api resources and grep a particular resource
k api-resources | grep -i "replicaset"

//Create namespace
k create namespace dev

//Get all namespace
k get ns --no-headers| wc -l

//Switch namespace permanently to some ns 
k config set-context $(kubectl config current-context) --namespace=dev

//Namespacing a resource - below is applicable for all the resource types
//you can also add namespace while creating resources in the metadata section next to name
//gets all the pods from default namespace
k get pods 
//gets all pods from all namespace
k get pods -A
k get pods --all-namespace
//gets all pods from a specified namespace
k get pods --namespace=kube-system
k get pods -n research --no-headers | wc -l
k get pods --selector env=dev --no-headers
k get pods --selector env=prod,bu=finance,tier=frontend --no-headers


//Create pod imperatively
k run redis --image=redis
k run nginx --image=nginx
k run redis --image redis -n finance
k run redis --image redis:alpine --labels='tier=db'

//Create Pod dry run only 
kubectl run nginx --image=nginx --dry-run=client -o yaml > <filename>.yaml
kubectl create deployment --image=nginx nginx --replicas=4 --dry-run -o yaml > <filename>.yaml

//Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379
kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml
//This will not use the pods labels as selectors, instead it will assume selectors as app=redis.
//So generate the file and modify the selectors before creating the service
kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml

//Create a Service named nginx of type NodePort to expose pod nginx’s port 80 on port 30080 on the nodes:
//This will automatically use the pod’s labels as selectors, but you cannot specify the node port. 
//You have to generate a definition file and then add the node port in manually before creating the service with the pod.
kubectl expose pod nginx --port=80 --name nginx-service --type=NodePort --dry-run=client -o yaml

//This will not use the pods labels as selectors
kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml

//Create deployment
k create deployment webapp --image=kodekloud/webapp-color --replicas=3

//Command to edit pods on the go
//Only listed properties below are editable
//spec.containers[*].image
//spec.initContainers[*].image
//spec.activeDeadlineSeconds
//spec.tolerations
//spec.terminationGracePeriodSeconds
k edit pod redis

//We can extract the pod definition file and edit it using the below Command
k get pod <pod-name> -o yaml > pod-definition.yaml

//After downloading the yaml in the above step, we can replace it using the below Command
k replace -f <yamlfilename>.yaml

//Create a kubernetes resource from a yaml file
k create -f <yamlfilename>.yaml

//To get a resource and its definition
k get <resourcetype> <resourcename>
k get pod nginx

//To delete a resource
k delete <resourcetype> <resourcename>
k delete pod nginx

//Scale a replicaset
k scale -replicas=6 -f <yamlfilename>.yaml

//when a service is created a dns entry is added automatically in the below format
db-service.dev.svc.cluster.local
//cluster.local - default domain name
//svc - sub domain for the service
//dev - namespace
//db-service - service name itself


//configmaps
//https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/
kubectl get configmaps
k describe configmaps <configmap-name>
k create configmap webapp-config-map --from-literal=APP_COLOR=darkblue --from-literal=APP_OTHER=disregard

//secrets
//https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/#define-container-environment-variables-using-secret-data
k get secrets
k describe secrets <secret-name>
k create secret generic db-secret --from-literal=DB_Host=sql01 --from-literal=DB_User=root --from-literal=DB_Password=password123


//Encrypting secret in ETCD
//https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/

//ETCD database backup
//this command will install the ETCDCTL
apt-get install etcd-client 

//check if already the encryption is enabled
ps -aux| grep kube-api | grep "encryption-provider-config"

//look at the manifest file
cat /etc/kubernetes/manifests/kube-apiserver.yaml

//follow steps in this guide to encrypt data at rest in ETCD DB
//https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/
//only secrets created newly after applying the encryption configuration will reflect this change 

//Service account
kubectl create serviceaccount dashboard-sa 
kubectl get serviceaccount
kubectl describe serviceaccount dashboard-sa

//To not mount the automatic service account in the pod set automountServiceAccountToken to false

//Make use of the kubectl set command. Run the following command to use the newly created service account
kubectl set serviceaccount deploy/web-dashboard dashboard-sa

//Taint and tolerations
//kubectl taint node nodename key=value:taint-effect
//tain-effect defines what will happen to the pod if they do not tolerate the taint
//There are three taint effects
// NoSchedule - New pods will not be scheduled if they dont tolerate 
// PreferNoSchedule - 
// NoExecute - New pods will be not be scheduled if they dont tolerate and also existing pods will be evicted if they dont tolerate

//To Taint
kubectl taint nodes node1 app=blue:NoSchedule

//To UnTaint
kubectl taint nodes node1 app=blue:NoSchedule-

//Toleration is added in the pod definition file. in the spec.tolerations
// spec.tolerations.key(app),spec.tolerations.operator(Equal), spec.tolerations.value(blue), spec.tolerations.effect(NoSchedule)

//Taint and Toleration doesnt tell pod to go to particular node, instead it tells nodes to accept particular pods that tolerates the taint
//To see the taint run the below command
kubectl describe node node1 | grep -i taint

//Node affinity helps restrict pod to certain node
//Node selector help control which node the pod is scheduled
//add it to pod definition i.e. spec.nodeSelector and spec.nodeSelector.key.value
//we need to label the node for the key value to work

//kubectl label nodes <node-name> <label-key>=<label-value>
//this has some limitations and hence node affinity was introduced

//Available node affinity values
//      requiredDuringSchedulingIgnoredDuringExecution
//      preferredDuringSchedulingIgnoredDuringExecution
//Planned node affinity values
//      requiredDuringSchedulingRequiredDuringExecution

//logging
// -f flag shows live logs
k logs -f <pod-name> 
//if multiple containers provide the container name
k logs -f <pod-name> <container-name>

//metrics server
//deploy metric server by cloning the deployment files
git clone https://github.com/kubernetes-incubator/metrics-serve
git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git
//
k create -f deploy/1.8+/
//after installation the below commands can be run
k top node
k top pod


//Now, in k8s version 1.20+ we can create an Ingress resource from the imperative way like this:-
//Format - kubectl create ingress <ingress-name> --rule="host/path=service:port"
//Example - kubectl create ingress ingress-test --rule="wear.my-online-store.com/wear*=wear-service:80"

//List of all commands - https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-ingress-em-

//ingress requires Ingress Controller which requires two service accounts with proper role and role bindings in place. Also requires a config map as well. 
//create a service to expose the ingress-controller deployment as a NodePort
//once the ingress controller is created, the ingress resource can be created

//use ingress-controller.yaml to create the ingress controller
//next expose the newly created controller via the service (note: this is not the ingress resource)
k expose deploy ingress-controller -n ingress-space --name ingress --port=80 --target-port=80 --type NodePort

//use the below command to create the ingress resource
k create ingress ingress-wear-watch -n app-space --rule="/wear=wear-service:8080" --rule="/watch=video-service:8080"
//add annotations rewrite-target and ssl-redirect if the url is not loading


//Docker has two types of mount - volume mount and bind mount 
//src will be local or third party storage like aws, afs, etc and target will be the location within the container which we want to be saved externally

//In Kubernetes world, create the volume first i.e. as simple as in the host and then mount it using the volumemount in the container spec

//We cant always create volume in each pod definition file and hence we have create the PV 
//Persistent Volume (PV) acts like a pool and whenever a Pod makes a PVC (Persistent Volume Claim) it provides the required storage to it
//this is the advantage over the volume-simple.yaml

//every PVC is bound to one PV only
//Basis on the below properties
// Capacity , Access modes, Volume Modes, Storage Class
//use labels and selectors for precision binding i.e PV will get label and PVC will get selector 
// 1to1 binding PV to PVC. If no PV is found, then PVC will wait for a PV to be available 

//When PVC is deleted, PV by default is set to retain, until it is manually set to delete by admin
//If PVReclaimPolicy set to Delete, then if PVC is deleted, then the PV is also deleted
//One more option is for PVReclaimPolicy is Recycle , data will be scrubbed in the storage location and made available to the other PVC

//refer volume-pvc.yaml to see how to mount the PVC to Pod
// sample yaml available at https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume

//Storage Classes
//In the PV we specify AWS, GCE or AZ File Shares, we might need to manually provision them everytime
//This manual provisioning is called static provisioning 

//Dynamic Provisioning is preferred 
//create a resource of Kind StorageClass 
//This StorageClass can replace PV and PV will not be required, instead in PVC we can directly specify storageClassName
//StorageClass creates the PV, we dont need to manually create the PV
// https://kubernetes.io/docs/concepts/storage/storage-classes/

//Stateful set
// If you need pods to come up in a sequential order and with a predictable name, then stateful set is used
// Its like Deployment only, with above differences
// In the Deployment yaml we can change Kind to StatefulSet and add a serviceName with a headless service name
// Ordered, Graceful deployment - StatefulSet
// Pods are deleted or bought down in the reverse order
// If PodManagementpolicy is set to Parallel, this ordered approach is not followed. Default value is ordered


//In General you need to not refer other pods by IP address, rather use hostname. If you want static hostname use the stateful set
//Using stateful set guarantees that the hostname is static eventhough the Pod crashes and comes up with new IP 

//Headless service 
// Usually service load balances the request among the pods under it, we cant reach a pod uniquely
// But a headless service gives a unique name to each pod, which is very useful to access specific pods only in stateful sets as pods cant be exposed but only via a service
// e.g. mysql-0.mysql-h.defaul.svc.cluster.local to reach mysql-0 pod under service mysql-h 
// ClusterIP: None will create a headless service in a normal service yaml file


//The below two subdomain and hostname are used in deployment as optional fields and they generate the same name for the Pod in DNS
// In Pod definition specify subdomain: as the name of the headless service . This creates a DNS record to the name of the service to point to the Pod
// To create A record for the Pod specify the hostname: option 

//But in a Stateful Set if we just specify the serviceName with the headless service name, then we can skip subdomain and hostname in the spec yaml
//StatefulSet create unique A records for each replica to be accessed individually 

//VolumeClaimTemplate is used in the Pod definition of deployment to automatically create unique PV for each Pod rather than creating manually PVC and using the same across the Pods
// pod created -> VolumeClaimTemplate creates a PVC -> PVC has a storage class which creates a PV -> PV is created in the provider as physical storage 

//If a Pod is deleted and recreated, then StatefulSet reattaches the Pod to the same PVC and ensures storage stability 