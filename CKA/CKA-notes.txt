echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

kubeadm | 1.29.0-1.1 | https://pkgs.k8s.io/core:/stable:/v1.29/deb  Packages


1.29.0-1.1

sudo apt-mark unhold kubeadm && \
sudo apt-get update && sudo apt-get install -y kubeadm='1.29.0-1.1' && \
sudo apt-mark hold kubeadm

//for master
sudo kubeadm upgrade apply v1.29.0

//for worker
sudo kubeadm upgrade node


sudo apt-mark unhold kubelet kubectl && \
sudo apt-get update && sudo apt-get install -y kubelet='1.29.0-1.1' kubectl='1.29.0-1.1' && \
sudo apt-mark hold kubelet kubectl


sudo systemctl daemon-reload
sudo systemctl restart kubelet

-------------------------------------------------------------------------

ETCD 
Server certificate -> /etc/kubernetes/pki/etcd/server.crt
ETCD CA Certificate -> /etc/kubernetes/pki/etcd/ca.crt

/opt/snapshot-pre-boot.db

echo ETCDCTL_API=3
printenv

ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /opt/snapshot-pre-boot.db

-------

To restore snapshot

ETCDCTL_API=3 etcdctl  --data-dir /var/lib/etcd-from-backup \
snapshot restore /opt/snapshot-pre-boot.db

Next, update the /etc/kubernetes/manifests/etcd.yaml

volumes:
  - hostPath:
      path: /var/lib/etcd-from-backup
	  
You can run the command: watch "crictl ps | grep etcd" to see when the ETCD pod is restarted.

----------
List nodes in etcd external server

etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/pki/ca.pem --cert=/etc/etcd/pki/etcd.pem --key=/etc/etcd/pki/etcd-key.pem \
  member list
---------------- 
Restoring in External ETCD server

Step 1. Copy the snapshot file from the student-node to the etcd-server. In the example below, we are copying it to the /root directory:
student-node ~  scp /opt/cluster2.db etcd-server:/root
cluster2.db                                                                                                        100% 1108KB 178.5MB/s   00:00    

Step 2: Restore the snapshot on the cluster2. Since we are restoring directly on the etcd-server, we can use the endpoint https:/127.0.0.1. Use the same certificates that were identified earlier. Make sure to use the data-dir as /var/lib/etcd-data-new:

etcd-server ~ ➜  ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/etcd/pki/ca.pem --cert=/etc/etcd/pki/etcd.pem --key=/etc/etcd/pki/etcd-key.pem snapshot restore /root/cluster2.db --data-dir /var/lib/etcd-data-new
{"level":"info","ts":1662004927.2399247,"caller":"snapshot/v3_snapshot.go:296","msg":"restoring snapshot","path":"/root/cluster2.db","wal-dir":"/var/lib/etcd-data-new/member/wal","data-dir":"/var/lib/etcd-data-new","snap-dir":"/var/lib/etcd-data-new/member/snap"}
{"level":"info","ts":1662004927.2584803,"caller":"membership/cluster.go:392","msg":"added member","cluster-id":"cdf818194e3a8c32","local-member-id":"0","added-peer-id":"8e9e05c52164694d","added-peer-peer-urls":["http://localhost:2380"]}
{"level":"info","ts":1662004927.264258,"caller":"snapshot/v3_snapshot.go:309","msg":"restored snapshot","path":"/root/cluster2.db","wal-dir":"/var/lib/etcd-data-new/member/wal","data-dir":"/var/lib/etcd-data-new","snap-dir":"/var/lib/etcd-data-new/member/snap"}

Step 3: Update the systemd service unit file for etcdby running vi /etc/systemd/system/etcd.service and add the new value for data-dir:

[Unit]
Description=etcd key-value store
Documentation=https://github.com/etcd-io/etcd
After=network.target

[Service]
User=etcd
Type=notify
ExecStart=/usr/local/bin/etcd \
  --name etcd-server \
  --data-dir=/var/lib/etcd-data-new \
---End of Snippet---

Step 4: make sure the permissions on the new directory is correct (should be owned by etcd user):

etcd-server /var/lib ➜  chown -R etcd:etcd /var/lib/etcd-data-new

etcd-server /var/lib ➜ 


etcd-server /var/lib ➜  ls -ld /var/lib/etcd-data-new/
drwx------ 3 etcd etcd 4096 Sep  1 02:41 /var/lib/etcd-data-new/
etcd-server /var/lib ➜

Step 5: Finally, reload and restart the etcd service.

etcd-server ~/default.etcd ➜  systemctl daemon-reload 
etcd-server ~ ➜  systemctl restart etcd
etcd-server ~ ➜

Step 6 (optional): It is recommended to restart controlplane components (e.g. kube-scheduler, kube-controller-manager, kubelet) to ensure that they don't rely on some stale data.
---------------- 

Generate private and public key using OpenSSL

Private Key
openssl genrsa -out mykey.key 1024

Public Key
openssl rsa -in mykey.key -pubout > mykey.pem 

----------------
Generate CSR via OpenSSL

openssl req -new -key mykey.key -out mycsr.csr -subj "/C=US/ST=CA/O=MyOrg,Inc./CN=my-bank.com"
----------------
Generate CSR for Admin User 

openssl req -new -key admin.key -subj "/CN=kube-admin/O=system:masters" -out admin.csr

## /O=system:masters specifies the group which will use this certificate
----------------
Generate CSR for API server 
openssl req -new -key apiserver.key -subj "/CN=kube-apiserver" -out apiserver.csr -config openssl.cnf

Create below openssl config file to pass in the above command. This is for the alternate names
***
openssl.cnf
[req]
req_extensions= v3_req
distinguished_name = req_distinguished_name
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation,
subjectAltName = @alt_names
[alt_names]
DNS.1 = kubernetes
DNS.2 = kubernetes.default
DNS.3 = kubernetes.default.svc
DNS.4 = kubernetes.default.svc.cluster.local
IP.1 = 10.96.0.1
IP.2 = 172.17.0.87
***
----------------
Sign certificate for API Server

openssl x509 -req -in apiserver.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out apiserver.crt -extensions v3_req -extfile openssl.cnf -days 1000
----------------
Sign Certificate for CA

openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt
----------------
Sign Certificate for User using CA 

openssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -out admin.crt 

----------------

Public Key - *.crt or *.pem

Private key - *.key or *-key.pem

----------------
Follow below steps to debug a certificate using openssl

Below command displays the properties of the certificate file
openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout
----------------

If cluster is setup from scratch search the native os logs for issues in certificates

journalctl -u etcd.service -l 

If cluster is setup using kubeadm, then components are deployed as pods, use below command

kubectl logs etcd-master -n kube-system

If api-server or cluster is down use docker to debug the logs

//list all the conatiners, pick the POD kube_system one
docker ps -a 

//view logs using docker logs
docker logs <container_id>
----------------
//List of commands for network troubleshooting 
//changes made using below command are only valid till a restart, if you want to persist set them in the /etc/network/interfaces file

//list and modify interfaces on the host (linux)
ip link 

//ip addresses assigned to the interfaces
ip addr

//set ip addresses on the interfaces
ip addr add 192.168.1.10/24 dev eth0

//to view the routing table
ip route 

//add entries into the routing table
ip route add 192.168.1.0/24 via 192.168.2.1

//command to check if ip forwarding is enabled or not is below
//if set manually in the below location, it persists only till restart. Should be set in the systemctl conf file 
cat /proc/sys/net/ipv4/ip_forward 

//DNS server info is stored in the below location
cat /etc/resolv.conf

//Host file acts like a local DNS 
cat /etc/hosts

//if server is present in both places, local host file takes priority and is defined in the below location
cat /etc/nsswitch.conf

//IPv4 to Hostnames - A record
//IPv6 to hostname - AAAA (quad A) record
//One hostname to another hostname - CNAME record


//DNS resolution test 
//nslookup ignores entry in the local host file
nslookup

//DNS server port is port 53
----------------
//Network Namespaces

//Create a new network Namespaces
ip netns add red 
ip netns add blue 

//List network Namespaces
ip netns

//ip link - this command is used to list Namespaces
//To do that in the namespace use the below commands
ip netns exec red ip link
ip -n red link 

//Same applies for the arp table
arp 
ip netns exec red arp

//Same applies for the routing table
route
ip netns exec red route

//Default gateway is availble using below command
ip route show default

//find ports listening in a server (which port is listended by whom)
netstat -nplt 

//find which port clients are listening more on etcd
netstat -anp | grep etcd 

----------------------------------
Use a JSON PATH query to identify the context configured for the aws-user in the my-kube-config context file and store the result in /opt/outputs/aws-context-name.

kubectl config view --kubeconfig=my-kube-config -o jsonpath="{.contexts[?(@.context.user=='aws-user')].name}" > /opt/outputs/aws-context-name

#Data Below
controlplane ~ ➜  kubectl config view --kubeconfig=/root/my-kube-config
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: KUBE_ADDRESS
  name: development
- cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: KUBE_ADDRESS
  name: kubernetes-on-aws
- cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: KUBE_ADDRESS
  name: production
- cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: KUBE_ADDRESS
  name: test-cluster-1
contexts:
- context:
    cluster: kubernetes-on-aws
    user: aws-user
  name: aws-user@kubernetes-on-aws
- context:
    cluster: test-cluster-1
    user: dev-user
  name: research
- context:
    cluster: development
    user: test-user
  name: test-user@development
- context:
    cluster: production
    user: test-user
  name: test-user@production
current-context: test-user@development
kind: Config
preferences: {}
users:
- name: aws-user
  user:
    client-certificate: /etc/kubernetes/pki/users/aws-user/aws-user.crt
    client-key: /etc/kubernetes/pki/users/aws-user/aws-user.key
- name: dev-user
  user:
    client-certificate: /etc/kubernetes/pki/users/dev-user/developer-user.crt
    client-key: /etc/kubernetes/pki/users/dev-user/dev-user.key
- name: test-user
  user:
    client-certificate: /etc/kubernetes/pki/users/test-user/test-user.crt
    client-key: /etc/kubernetes/pki/users/test-user/test-user.key

-----------------------------------


Upgrading Nodes via kubeadm

https://v1-29.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/

# Find the latest 1.29 version in the list.
# It should look like 1.29.x-*, where x is the latest patch.
sudo apt update
sudo apt-cache madison kubeadm


# replace x in 1.29.x-* with the latest patch version
sudo apt-mark unhold kubeadm && \
sudo apt-get update && sudo apt-get install -y kubeadm='1.29.0-1.1' && \
sudo apt-mark hold kubeadm

#Verify that the download works and has the expected version:
kubeadm version

#Verify the upgrade plan:
sudo kubeadm upgrade plan

#Choose a version to upgrade to, and run the appropriate command. For example:
# replace x with the patch version you picked for this upgrade
sudo kubeadm upgrade apply v1.29.x

-------------------------------

#Drain the node 
#Prepare the node for maintenance by marking it unschedulable and evicting the workloads:

# replace <node-to-drain> with the name of your node you are draining
kubectl drain <node-to-drain> --ignore-daemonsets

#Upgrade the kubelet if the node is not reflecting the latest version
# replace x in 1.29.x-* with the latest patch version
sudo apt-mark unhold kubelet kubectl && \
sudo apt-get update && sudo apt-get install -y kubelet='1.29.0-1.1' kubectl='1.29.0-1.1' && \
sudo apt-mark hold kubelet kubectl

#Restart the kubelet:
sudo systemctl daemon-reload
sudo systemctl restart kubelet


#Uncordon the node 
#Bring the node back online by marking it schedulable:
# replace <node-to-uncordon> with the name of your node
kubectl uncordon <node-to-uncordon>

-------------------------------

#Take the backup of ETCD at the location /opt/etcd-backup.db on the controlplane node.
etcdctl snapshot save --help

export ETCDCTL_API=3
etcdctl snapshot save --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --endpoints=127.0.0.1:2379 /opt/etcd-backup.db

-------------------------------
Create pod from imperative command

kubectl run secret-1401 -n admin1401 --image=busybox --dry-run=client -oyaml --command -- sleep 4800 > admin.yaml

-------------------------------

One time pod for testing connectivity

kubectl run busybox -it --image=busybox:1.28 --restart=Never --rm -- nslookup nginx-resolver-service

-------------------------------

Check service account permissions

kubectl auth can-i <verb> <resource> --as=system:serviceaccount:<namespace>:<serviceaccountname> [-n <namespace>]

List all permissions for service account

kubectl auth can-i --as=system:serviceaccount:default:default --list

-------------------------------

Command to check top node

 kubectl top node --context cluster2 --no-headers | sort -nr -k2 | head -1

 -------------------------------

Troubleshooting cluster 

#Check the kubelet logs for the error if etcd pod is not coming up as below 
journalctl -u kubelet -f

#Restart kubelet as below 
systemctl restart kubelet

#Check the pod logs for the warning on the health probes (live and ready), sometimes port might be wrong

-------------------------------
etcd backup as below
ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key snapshot save /opt/etcd-boot-cka18-trb.db

-------------------------------
#Endpoints 
export IP_ADDR=$(ifconfig eth0 | grep inet | awk '{print $2}')

apiVersion: v1
kind: Endpoints
metadata:
  # the name here should match the name of the Service
  name: external-webserver-cka03-svcn
subsets:
  - addresses:
      - ip: $IP_ADDR
    ports:
      - port: 9999

-------------------------------