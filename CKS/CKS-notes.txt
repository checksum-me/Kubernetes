Entire course is available here -> https://github.com/kodekloudhub/certified-kubernetes-security-specialist-cks-course/tree/main/docs

###Kubelet Security
#Kubelet will run as a service or if setup via Kubeadm then the config file will have the config details
#Kubelet Service location -> /usr/local/bin/kubelet in /etc/systemd/system/kube-apiserver.service
#Kubelet config file -> /var/lib/kubelet/kubelet-config.yaml

#Kubelet Port Access
10250 - Serves API that allows full Access
10255 - Serves API that allows unauthenticated read Access

curl -sk https://localhost:10250/pods/
curl -sk https://localhost:10250/logs/syslog
curl -sk http://localhost:10255/metrics

#Set the below to false in the config file
--anonymous-auth=false
#otherwise the above urls will allow anyone to read the kubelet status

#Kubelet support two authentication mechanism - Certificate based and Bearer token based

#For certificate based authentication
--client-ca-file=/path/to/ca.crt or clientCAFile: /path/to/ca.crt

#Default kubelet allows all requests from API server without auth, to set auth use auth-mode to Webhook as below
#When set kubelet authorises req via kube-apiserver
--authorization-mode=Webhook or authorization: mode: Webhook

#Read only port by default is set to 10255 which enables to read metrics server without auth, set to 0 to disable this as below
--read-only-port=0 or readOnlyPort: 0

#Check kubelet config file
ps -aux | grep kubelet

#Kubelet config needs to be edited to change auth mode
vi /var/lib/kubelet/config.yaml
#This is one setting
authentication:
  anonymous:
    enabled: false
#Once above change is done, you should get Unauthorized
controlplane ~ ➜  curl -sk https://localhost:10250/pods
Unauthorized

#This is another setting
authorization:
  mode: Webhook
#Once above change is done, it will not allow to connect as anonymous

#This is another setting
#Set readOnlyPort as 0 in /var/lib/kubelet/config.yaml and restart kubelet
#The metrics URL wont work

#Restart kubelet service to take effect 
systemctl restart kubelet.service
systemctl status kubelet.service

curl -sk https://localhost:10250/pods
curl -sk http://localhost:10255/metrics

###Kubectl Proxy and Port forward
kubectl proxy - Opens proxy port to API server
kubectl port-forward - Opens port to target deployment pods
##Kubectl Proxy
#The below command starts a proxy server to the API server - default port is 8001
#Once the proxy server is started, we can connect to it using curl command locally via localhost
#To run the process in background you can use & operator at the end

kubectl proxy &
#To run on a different port
kubectl proxy --port 8002 &
curl 127.0.0.1:8001/version

##Port forward
#Port from out laptop 28080 is forwarded to the 80 port of the service
kubectl port-forward service/nginx 28080:80
kubectl port-forward pod/nginx-77d8468669-52jpx 8005:80 &
#The below command can access the service
curl http://localhost:28080/

###Kubernetes Dashboard
#View and create new apps. Can do anything
#To access in local laptop create proxy and open the below link if dashboard is setup in cluster
kubectl proxy
#Open below link in browser
https://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy

#Since the dashboard is clusterip its not accessible outside cluster
#Can be made loadbalancer or nodeport

##Dashboard authentication
#Can use Token or KubeConfig file
#For token , create user and assign RBAC permission, generate token and enter in website

#Run below command to create dashboard
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml

#Now run kubectl proxy with the below command 
kubectl proxy --address=0.0.0.0 --disable-filter &

#Note: From a security perspective do not use --disable-filter option as it can leave you vulnerable to XSRF attacks, when used with an accessible port. We have used this option to make our lab environment work with the kubernetes dashboard so you can access it through a browser. Ideally you would be accessing it through a kubectl proxy on your localhost only.
#So in actual environments do not use --disable-filter option as its a major security risk.

#Get token by running below and login to UI with that token:
kubectl get secrets -n kubernetes-dashboard admin-user -o go-template="{{.data.token | base64decode}}"

#Below are some references:
https://redlock.io/blog/cryptojacking-tesla
https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/
https://github.com/kubernetes/dashboard
https://www.youtube.com/watch?v=od8TnIvuADg 
https://blog.heptio.com/on-securing-the-kubernetes-dashboard-16b09b1b7aca
https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md

###Verify the platform binaries
#Important to verify hash of the downloaded file

#Download file using curl command
curl https://dl.k8s.io/v1.20.0/kubernetes.tar.gz -L -o kubernetes.tar.gz
#Verify the files hash generated from below command with the one is download page
#shasum generates hash for the downloaded file
shasum -a 512 kubernetes.tar.gz #MacOS
sha512sum kubernetes.tar.gz #Linux

#To decompress a file
cd /opt/
tar -xf kubernetes.tar.gz
#Make some changes
cd kubernetes
echo "v1.30.0-modified" > version
cd ..
#compress the directory again
tar -czf kubernetes-modified.tar.gz kubernetes
shasum -a512 kubernetes-modified.tar.gz

###Below are some references for kubernetes versioning and different releases
https://github.com/kubernetes/kubernetes/releases
https://github.com/kubernetes/design-proposals-archive/blob/main/release/versioning.md
https://github.com/kubernetes/design-proposals-archive/blob/main/api-machinery/api-group.md
https://blog.risingstack.com/the-history-of-kubernetes/
https://kubernetes.io/docs/setup/version-skew-policy

###Kubernetes upgrade in controlplane and worker nodeport

###Network policies

###Ingress and how to setup the same 

###Configuring docker service

systemctl start docker 
systemctl status docker 
systemctl stop docker 
#configuring the service loads it up automatically when the system loads up
#Also can be loaded manually in foreground using dockerd
dockerd --debug
#This is for troubleshooting docker. --debug adds more details

#when docker starts it listens to internal unix socket at /var/run/docker.sock
#Unix socket is for IPC - Inter Process Communication 
# Used for communication between different processes on the same host
#docker.sock is only accessible from docker CLI on the same host
#standard port for docker is 2375

dockerd --debug --host=tcp://192.168.1.10:2375
#the host will make docker available on the mentioned IP
#need to be extremely careful while doing this as this exposes docker outside of host. mght allow anyone on the internet to run containers on this host
#docker default setting is its not encrypted and no authentication is required

#In the another host export a DOCKER_HOST and then you can access docker as below
export DOCKER_HOST="tcp://192.168.1.10:2375"
docker ps
#default is unencrypted traffic

#To enable encryption set tls to true and add certificates
dockerd --debug \
        --host=tcp://192.168.1.10:2376 #Note 2376 is the secure https port compared to 2375 unencrypted port
        --tls=true \
        --tlscert=/var/docker/server.pem \
        --tlskey=/var/docker/serverkey.pem

#move config to a json file at /etc/docker/daemon.json
{
  "debug": true,
  "hosts": ["tcp://192.168.1.10:2376]
  "tls": true,                              #Only enables encryption does not enforce TLS authentication
  "tlscert": "/var/docker/server.pem",
  "tlskey": "/var/docker/serverkey.pem"
  "tlsverify": true,                        #This is configured to enforce TLS authentication
  "tlscacert": "/var/docker/caserver.pem"   #This is configured to enforce TLS authentication
}

#once config file is provided no need to pass options in the command dockerd
#These options are passed through in the docker via background also via systemctl
#In the client machine place the certs in ./docker users home directory or pass in the CLI

###Debugging a crashed API server
https://github.com/kodekloudhub/community-faq/blob/main/docs/diagnose-crashed-apiserver.md

###Securing controlplane and etcd 
#Restrict communication between etcd and api server to the cipher TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 and also restrict the api server minimum TLS version to TLS 1.2
#Edit the API server manifest and add the following two arguments
--tls-min-version=VersionTLS12
--tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
#Edit the etcd manifest and add the following argument
--cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
#Wait for both pods to restart. This may take a minute or more.


###System Hardening 

##Limiting the node access
#Four types of accounts - User, SuperUser, System and Service accounts
#id command gives info about the user
id #currentuser
id michael #michael info
#who command to see list of users logged into the system
who
#last command lists the last time users were logged into the system
last 
##Access control files
#/etc/password - contains the id, uid, gid details of user
#/etc/shadow - password is stored here. contents are hashed 
#/etc/group - stored info of all the user groups

#We can disable user by changing shell to nologin
usermod -s /bin/nologin michael
#we can delete the user account itself using below command
userdel bob 
#To create user use below command
useradd -d /opt/sam -s /bin/bash -G admin -u 2328 sam
#to delete group
groupdel <groupname>
#To remove from the group use command below
deluser michael admin
#To change password for user below command is used
passwd <username> #prompt will ask for new password

##SSH Hardening
ssh <hostname or IP adddress>
ssh <user>@<hostname or IP adddress>
ssh -l <user> <hostname or IP adddress>
#Port 22 should be open for SSH

#Generate the private and public key pair
#public key is installed on the remove server. Client connects using private key
#passphrase is optional
ssh-keygen -t rsa 
#public key is stored in usres home directory : /home/mark/.ssh/id_rsa.pub 
#Private key : /home/mark/.ssh/id_rsa 

#Next copy the public key to remove server. Password based auth is used once
ssh-copy-id mark@node01

#Public keys should installed in /home/mark/.ssh/authorized_keys

##SSH Hardening
#Disable ssh for root accounts 
vi /etc/ssh/sshd_config
#PermitRootLogin no - this will disable root login 
#PasswordAuthentication no - this will disable password login and allow only ssh via private key
#Once changes are done restart sshd service as below
systemctl restart sshd 

#CIS benchmark lists
https://www.cisecurity.org/cis-benchmarks

###Sudoers - Privilege Escalation
/etc/Sudoers
#Users listed above only can perform sudo actions
#Commands executed uring sudo are executed in the users own shell and not root shell

#To eliminate root user login do the below and set root to nologin
vi /etc/passwd
#root:x:0:0:/root:/usr/sbin/nologin

#Add user and setup ssh login
#ssh into node01 host from controlplane host
ssh node01

#Create user jim on node01 host
adduser jim (set any password you like)

#Return back to controlplane host and copy ssh public key
ssh-copy-id -i ~/.ssh/id_rsa.pub jim@node01

#Test ssh access from controlplane host
ssh jim@node01

#To enable passwordless sudo for a user, below is the command
#change below
jim ALL=(ALL:ALL) ALL
#To below
jim ALL=(ALL) NOPASSWD:ALL

###Remove Obsolete packages and Unwanted services
#List all services
systemctl list-units --type service 

#to stop unwanted service
systemctl stop apache2
systemctl disable apache2

#to remove the package then use below command
apt remove apache2

### Restrict Kernel Modules

#to load module for pc speaker use below command
modprobe pcspkr 

#To list all modules loaded into the Kernel
lsmod 

#To balcklist module add it in the /etc/modprobe.d/*.conf 
cat /etc/modprobe.d/blacklist.conf

#Reboot the node to pick up changes
shutdown -r now 

#load modules and check it should not be there 
lsmod | grep sctp 

##Disable open ports
#List all open ports
netstat -an | grep -w LISTEN 

#To check an unknown port in Ubuntu check the services file
cat /etc/services | grep -w 53 

https://kubernetes.io/docs/reference/networking/ports-and-protocols/

#List all installed packages in Ubuntu
apt list --installed

#Identify the service listening on port 9090
netstat -natp  | grep 9090

###UFW - Uncomplicated FireWall

#Install and start UFW
apt-get update
apt-get install ufw 
systemctl enable ufw 
systemctl start ufw

#status
ufw status

ufw default allow outgoing
ufw default deny incoming

#next add allow rules
ufw allow from 172.16.238.5 to any port 22 proto tcp
ufw allow from 172.16.238.5 to any port 80 proto tcp

ufw allow from 172.16.100.0/28 to any port 80 proto tcp

ufw deny 8080

#Activate the FireWall
ufw enable 

#Status of the FireWall
ufw status

#Delete rules
ufw delete deny 8080 
#or provide the line number from status
ufw delete 5 

#Allow tcp ports from 1000 to 2000
ufw allow 1000:2000/tcp

#reset to default settings
ufw reset

#enable port 22 for SSH
ufw allow 22
ufw deny 80

###Linux Syscalls

#Tracing syscalls 
#strace is available in most linux distro and helps traces syscalls made by programs
which strace -> /usr/bin/strace 

#To inspect the command add strace in front of it. Lot of details will be displayed
strace touch /tmp/error.log 

#first check the pid of the process
pidof etcd -> 3596 (pid of the etcd)

#now attach strace to this pid, all future syscalls will be reported here 
strace -p 3596 

# flag -c will give more details in table format
strace -c touch /tmp/error.log 

#Tracee (opensource) tool by Aquasec uses ebpf to trace syscalls
#Can be install also or run as docker container
#if docker below bind mounts are required
# /tmp/tracee - Default workspace
# /lib/modules - Kernel Headers
# /usr/src - Kernel Headers

#The below traces the command ls and the syscalls made by it 
docker run --name tracee --rm --privileged --pid=host \
  -v /lib/modules/:/lib/modules/:ro -v /usr/src:/usr/src:ro \
  -v /tmp/tracee:/tmp/tracee aquasec/tracee:0.4.0 --trace comm=ls 

#Trace syscalls for all new processes 
# --trace container=new for all new containers syscalls 
sudo docker run --name tracee --rm --privileged --pid=host \
  -v /lib/modules/:/lib/modules/:ro -v /usr/src:/usr/src:ro \
  -v /tmp/tracee:/tmp/tracee aquasec/tracee:0.4.0 --trace pid=new 

#Around 435 Syscalls in linux 
#Seccomp is part of linux kernel introduced in 2005 

#Check if seccomp is supported by kernel by below command
grep -i seccomp /boot/config-$(uname -r)
#If output contains CONFIG_SECCOMP=y then it has Seccomp enabled

#Seccomp has 3 modes 
#Mode 0 - Disabled 
#Mode 1 - Strict - Read, write on syscalls
#Mode 2 - Filtered (selectively)

#To know which profile seccomp is running use below 
grep Seccomp /proc/1/1status 
#Output will be like -> Seccomp: 2

#Docker has built in seccomp filter 
#There is a json file and has whitelisted syscalls and rest all are blocked. This is highly restrictive and secure 
#There is blacklist json as well. Listed syscalls are blacklisted and aloows the rest - more susceptible to attacks

#Default seccomp of the docker blocks around 60 of 300+ syscalls in the x86 architecture

#To use custom profile in docker use the below
docker run -it --rm --security-opt seccomp=/root/custom.json docker/whalesay /bin/sh 

#Dont use --security-opt seccomp=unconfined as it will block all syscalls 

##Kubernetes does not implement seccomp by default 

#Security context section of the pod can be used to add the seccomp profile 
#/var/lib/kubelet/seccomp - default seccomp profile for kubernetes 
#Localhost profile should be loaded relative to this -> /var/lib/kubelet/seccomp/profiles/violation.json 
#File can contain default action like below 
{
  "defaultAction": "SCMP_ACT_ERRNO"
}

#Provide the above file as below in the pod security context
securityContext: 
  seccompProfile:
    type: Localhost 
    localhostProfile: profiles/violation.json 

#List of syscalls is stored in the below file 
grep -w 35 /usr/include/asm/unistd_64.h 

###App Armour 

##Linux security feature which confines programs to limited set of resources 

#check is apparmor is installed and running 
systemctl status apparmor 

#Check if the apparmor is enabled in kernel 
cat /sys/module/apparmor/parameters/enabled 

#Value of "Y" implies the app armour kernel module is loaded 

#Just like seccomp apparmor is applied to application via profile 
#profiles can be verified in the kernel 
cat /sys/kernel/security/apparmor/profiles 

#Check what apparmor profiles are loaded 
aa-status 

#Profiles can be loaded in 3 different modes - enforce, complain and unconfined 

apt-get install -y apparmor-utils 

#To start profiling an application or process run aa-genprof command
#The below command is profiling the script add_data.sh 
aa-genprof /root/add_data.sh 

#Once rpofiling is completed, new profiles along with existing one are stored in below location
cat /etc/apparmor.d/root.add_data.sh 

#To load a profile use below command, if nothing is retured then profile is successfully loaded 
apparmor_parser /etc/apparmor.d/root.add_data.sh 

#To disable use -R flag and create symlink 
apparmor_parser -R /etc/apparmor.d/root.add_data.sh 

ln -s /etc/apparmor.d/root.add_data.sh /etc/apparmor.d/disable/

#To use apparmor in pod add as an annotation to the pod metadata 

#To see which linux capabilities a command needs use the below command
getcap /usr/bin/ping 
#output -> /usr/bin/ping = cap_net_raw+ep 

#To get capabilities of a process use the below command
ps -ef | grep /usr/sbin/sshd | grep -v grep 
#root 779 (process number)
getpcaps 779 

#Docker gives only 14 capabilities to the containers 

#To load an apparmor profile 
apparmor_parser -q /etc/apparmor.d/usr.sbin.nginx 

##Open Policy Agent (OPA)
#Works as authorization and as part of Admission controller
#Language used to write OPA policy is Rego

#OPA gatekeeper in kubernetes - Installation steps
https://open-policy-agent.github.io/gatekeeper/website/docs/install/

#With OPA Gatekeeper, you can best practices and organizational conventions for your resources in your kubernetes cluster.
#For example, you can enforce policies like:
#All namespaces must have a label that lists a point-of-contact
#All images must be from approved repositories
#All pods must have resource limits

#How to create constraint templates - https://open-policy-agent.github.io/gatekeeper/website/docs/howto/#constraint-templates

#Gatekeeper library has list of Constraints that can be created -> https://open-policy-agent.github.io/gatekeeper-library/website/

##Below are some references:
#How Netflix Is Solving Authorization Across Their Cloud [I] – Manish Mehta & Torin Sandall, Netflix
https://www.youtube.com/watch?v=R6tUNpRpdnY
#OPA Deep Dive
https://www.youtube.com/watch?v=4mBJSIhs2xQ
#Dive deep into the world of Kubernetes security with our comprehensive guide to Secret Store CSI Driver.
https://www.youtube.com/watch?v=MTnQW9MxnRI

###Container Sandboxing
##gVisor
#gVisor has Sentry and Gofer
#Acts as intermediate between container and Kernel
#All syscalls happen via gVisor
# Each container has its own gVisor kernel acting as the middleman between the application and the linux kernel
#Not all apps might work with gVisor, each application needs to be tested to check compatibility 
#Disadvantage is the application might become slightly slower as there is a middleman

##Kata Container
#Has its own Kernel for each container. Acts like a light weight virtual machine
#Performance might be slightly dipped and each container might need slightly more memory and ram 
#Needs hardware virtualization support
#Needs nested virtualization and not supported by most cloud providers
#GCloud supports nested virtualization. But performance is poor 

#docker run --runtime kata -d nginx 
#docker run --runtime runc -d nginx

#handler runsc for gVisor and kata for kata
#Create a RunTimeClass Object to be used in the pod definition spec section

#kubernetes cluster uses containerd as the runtime. Containerd makes use of runc to start containers.

##One way SSL - Only User verifies the Bank website SSL, the bank doesnt verify the Users authenticity. Only the credentials are verified by the bank 

##Mutual TLS - both client and the server both verify themselves 

##Istio and linkerd supports mTLS encryption between pods
#Let the application talk as usual. But use Istio to encrypt communication 
#Istio runs as a sidecar and does encryption and decryption 

#Permissive or Opportunistic mode - enables both encrypted and plain transmission 
#Enforced or Strict mode - Only allows encrypted traffic 

#Protection and risk of using secrets
https://kubernetes.io/docs/concepts/configuration/secret/

##Also, the way Kubernetes handles secrets. Such as:
#A secret is only sent to a node if a pod on that node requires it.
#Kubelet stores the secret into a tmpfs so that the secret is not written to disk storage.
#Once the Pod that depends on the secret is deleted, kubelet will delete its local copy of the secret data as well.

#Having said that, there are other better ways of handling sensitive data like passwords in Kubernetes, such as using tools like Helm Secrets, HashiCorp Vault.


##Base vs Parent images
#When image is built FROM SCRATCH it is called Base image
#Build modular images, separate for webserver, separate for DB, etc 
#Container are ephemeral in nature. Store data in external volume. Persist data
#Official image or verified publisher are original images. Use up to date images
#Keep size of image small as possible 
#Remove unnecessary packages from images 
#Distroless docker images only contain application and runtime dependencies 
#Does not contain - package managers, shells, network tools, text editors, and other unwanted programs 

# image: docker.io/library/nginx - library is the default account on which system images are published in docker 
# image: <registry>/<useraccount>/<image-repo>
# image: <yourname>/<yourcompanyname>/nginx 

##create a secret of type docker-registry for kubernetes to access private registry for images 
##update imagepullsecrets section to use this docker-registry secret 
##kubelet uses these credentials to pull images from private repo 

##kubesec can be used for static analysis 
#Installation link -> https://kubesec.io/

wget https://github.com/controlplaneio/kubesec/releases/download/v2.13.0/kubesec_linux_amd64.tar.gz
tar -xvf  kubesec_linux_amd64.tar.gz
mv kubesec /usr/bin/

# kubesec can be installed locally and scanned using binary 
# kubesec scan pod.yaml 

#Or kubesec online service can be used to scan 
# curl -sSX POST --data-binary @"pod.yaml" https://v2.kubesec.io/scan 

##CVE - Common Vulnerabilities and Exposures 
#CVE Severity scores - 0 - lowest and 10 - highest (critical)
#trivy is a common tool used for vulnerability scanning 
https://github.com/aquasecurity/trivy

#pull an image 
crictl pull public.ecr.aws/docker/library/python:3.12.4

#Scan and save results to a location 
trivy image --output /root/python_12.txt public.ecr.aws/docker/library/python:3.12.4
trivy image --input alpine.tar --output alpine.json --format json 

##Lab - API Server 
kubectl config view -o yaml

#Automatically set the cluster name:
export CLUSTER_NAME=$(kubectl config view -o jsonpath='{.clusters[0].name}')
echo $CLUSTER_NAME
#Retrieve the API server address:
APISERVER=$(kubectl config view -o jsonpath="{.clusters[?(@.name==\"$CLUSTER_NAME\")].cluster.server}")
echo $APISERVER
# Alternative
kubectl config view -o yaml | grep server

#Get the authentication token:
# Get the default token secret name
SECRET_NAME=$(kubectl get secrets -n kube-system | grep 'bootstrap-token-*' | awk '{print $1}')

# Get the token ID and secret
TOKEN_ID=$(kubectl get secret -n kube-system $SECRET_NAME -o jsonpath='{.data.token-id}' | base64 --decode)
TOKEN_SECRET=$(kubectl get secret -n kube-system $SECRET_NAME -o jsonpath='{.data.token-secret}' | base64 --decode)

# Generate the token: <token-id>.<token-secret>
TOKEN="$TOKEN_ID.$TOKEN_SECRET"
echo $TOKEN
# Store the token in a file
echo $TOKEN > /root/token.txt
#Access the API directly using curl:
curl -X GET $APISERVER/api --header "Authorization: Bearer $TOKEN" --insecure


#Find and terminate the kubectl proxy process:
pkill -f "kubectl proxy"

#Alternatively, you can manually find the process ID and kill it:
ps aux | grep 'kubectl proxy' | grep "8090"
kill <process_id>

##Lab Bootstrap token use 
#Generate the Join Command Using the Bootstrap Token
kubeadm token create [random-token-id].[random-secret] --dry-run --print-join-command --ttl 2h
#This command will output the kubeadm join command with the necessary token and CA certificate hash

##Configuring API server to use ABAC (Attribute Based Access Control ) mode 
# /etc/kubernetes/manifests/kube-apiserver.yaml

- --authorization-mode=Node,RBAC,ABAC
- --authorization-policy-file=/etc/kubernetes/abac/abac-policy.jsonl

https://kubernetes.io/docs/reference/access-authn-authz/abac/
https://jsonlines.org/examples/
https://raw.githubusercontent.com/kubernetes/kubernetes/refs/tags/v1.31.0/pkg/auth/authorizer/abac/example_policy_file.jsonl

##Setting Up kubectl Credentials for the Service Account
#Create Secret and service accounts 
    apiVersion: v1
    kind: Secret
    metadata:
      name: john-secret
      namespace: default
      annotations:
        kubernetes.io/service-account.name: john
    type: kubernetes.io/service-account-token
    ---
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: john
      namespace: default
    secrets:
      - name: john-secret

#Step 1: Set up new credentials:
#Get token from the newly created secret above
kubectl config set-credentials john --token=$SA_TOKEN
#Step 2: Set up the new context:
kubectl config set-context john-context --cluster=kubernetes --namespace=default --user=john
kubectl config use-context john-context

##Istio 
#Releases page 
https://istio.io/latest/docs/setup/additional-setup/download-istio-release/

  ISTIO_VERSION=1.23.1 # e.g. 1.16.0
  curl -L https://istio.io/downloadIstio | ISTIO_VERSION=$ISTIO_VERSION sh -
  cd istio-$ISTIO_VERSION
  mv bin/istioctl /usr/local/bin/
#Install Istio base components 
  istioctl install --set profile=default -y

#To label the default namespace for Istio's automatic sidecar injection, run:
  kubectl label namespace default istio-injection=enabled

#To check if the pod is accessible from another pod use wget 
wget -qO- http://<podip>
wget -qO- <podip>

##Priority 
#API Priority - Fair processing of API requests 
#PriorityLevelConfiguration helps set prioirty to any API resources
#FlowControl created targets the resource from the PriorityLevelConfiguration
#Suppose NamespaceA needs to be prioritized over NamespaceB, then PriorityLevelConfiguration needs to be set to high (10) for NamespaceA

#Pod Priority - Ensures Critical Pods are scheduled and run 
#For Pods, create PriorityClass resources separately 
#Associate PriorityClass to the Pod via PriorityClassName 


##DNS for multi-tenant environment 
#Usually CoreDNS resolved all pods to all pods in a cluster
#Suppose we want to restrict traffic to namespace and not allow one ns to talk to other in a multi-tenant env, then below is the code to be added in the configmap of the coredns 

kubectl edit configmap coredns -n kube-system 

//Add the "fallthrough in-namespace" option in the configmap for coredns

##Pod to Pod security 
#Implemented by the below options
#mTLS via a service mesh like Istio
#Cilium - IPSec or Wideguard
#Calico - IPSec 

###Cilium 
##Opensopurce software for providing and securing network connectivity between container applications 
##Designed for modern microservices, it provides network security through extended Berkely Packet Filter (eBPF)

##Setting up Cilium 
#Installation , Configuration, Key Management, Policy Definition, Monitoring 

#Helps with encryption , IDtty management and policy enforcement 

# Add the Cilium Helm repository
helm repo add cilium https://helm.cilium.io/

# Install Cilium with encryption enabled
helm install cilium cilium/cilium --version 1.16.3 \
  --namespace kube-system \
  --set encryption.enabled=true \
  --set encryption.type=wireguard

# Wait for the status of cilium to be OK
cilium status

# Check the encryption status of the Cilium installation
cilium encryption status

#WireGuard is a modern, open-source VPN (Virtual Private Network) protocol designed to provide fast, secure, and efficient encrypted communication. It uses state-of-the-art cryptography to secure network traffic. In the context of Kubernetes, tools like Cilium integrate WireGuard to encrypt pod-to-pod traffic across nodes, enhancing network security by ensuring that data transmitted between pods is protected from interception or tampering.

#Check connectivity between the pods, in a new terminal window run the following command:
watch kubectl exec -it curlpod -- curl -s http://nginx

#The watch curl command should return the HTML content of the NGINX welcome page, indicating that the client pod can access the NGINX pod.

#Run a bash shell in one of the Cilium pods with kubectl -n kube-system exec -ti ds/cilium -- bash and execute the following commands:

#Check that WireGuard has been enabled (number of peers should correspond to a number of nodes subtracted by one):
cilium-dbg status | grep Encryption
#Install tcpdump
apt-get update
apt-get -y install tcpdump

#Check that traffic is sent via the cilium_wg0 tunnel device is encrypted:
tcpdump -n -i cilium_wg0 -X

#Here we are using `tcpdump`` to capture and display detailed network packets on the cilium_wg0 interface.
#The -n option avoids DNS lookups, and the -X option shows packet content in both hexadecimal and ASCII format.
#Via tcpdump, you should see the traffic between the pods.
#We see requests from curlpod to nginx and responses from nginx to curlpod in tcpdump output.

##Software Bill Of Materials (SBOM)
#SPDX and CycloneDX
#SPDX - Standard format for sharing software bill of materials 
#Complex due to extensive metadata 

#CycloneDX - Lightweight SBOM format designed for security and compliance 
#Simpler and more focused on essential SBOM elements 

#Generate -> Store -> Scan -> Analyze -> Remediate -> Monitor 

#Syft can be used to generate SBOM 
#Github can be used to store the SBOM 
#grype can be used to scan the SBOM 

#Install Syft 
curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b /usr/local/bin

#Scanning an image
syft docker.io/kodekloud/webapp-color:latest -o spdx > /root/webapp-spdx.sbom

#Grype Tool Installation 
curl -sSfL https://raw.githubusercontent.com/anchore/grype/main/install.sh | sh -s -- -b /usr/local/bin

#Analyze the /root/webapp-sbom.json SBOM using Grype to generate a vulnerability report and store it in /root/grype-report.json
grype sbom:/root/webapp-sbom.json -o json > /root/grype-report.json

#For continuous CI/CD and view the syft generated SBOM report, please refer below link 
https://github.com/checksum-me/supply_chain_security
#Refer the github actions tab for the run and the report as well 

##KubeLinter 
#Make static analysis of the YAML files for any improvements and suggestions 
#Check yaml folder for how to do kubelint via github actions 

#Download the latest version of KubeLinter for Linux using the command below:
curl -LO https://github.com/stackrox/kube-linter/releases/latest/download/kube-linter-linux.tar.gz
#Extract the binary from the tar file:
tar -xvf kube-linter-linux.tar.gz
#Move the binary to the /usr/local/bin/ path:
mv kube-linter /usr/local/bin/

